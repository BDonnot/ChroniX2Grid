{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebooks generate the data that will help the calibration of an environment and its analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will be able to load a grid environment with existing chronics and get data out of it on electrical flows. This will allow you to study the flows, their thermal limits and resulting congestions, to either calibrate thermal limits or study the level of difficulty of the environment.\n",
    "\n",
    "As an output of this notebook, you will get a large dataframe of flows and other grid state information over all run scenarios (with do nothing agent, and possibly other basic agents). If thermal limits already exist for your environment, you can afterwards generate a summary dataframe on observed congestions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#This-notebooks-generate-the-data-that-will-help-the-calibration-of-an-environment-and-its-analysis\" data-toc-modified-id=\"This-notebooks-generate-the-data-that-will-help-the-calibration-of-an-environment-and-its-analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>This notebooks generate the data that will help the calibration of an environment and its analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Config-parameters-for-data-generation\" data-toc-modified-id=\"Config-parameters-for-data-generation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Config parameters for data generation</a></span></li><li><span><a href=\"#Load-the-environment\" data-toc-modified-id=\"Load-the-environment-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load the environment</a></span></li><li><span><a href=\"#Generate-the-data\" data-toc-modified-id=\"Generate-the-data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Generate the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Run-basic-agents-on-scenarios-and-get-flow-results-in-dataframe\" data-toc-modified-id=\"Run-basic-agents-on-scenarios-and-get-flow-results-in-dataframe-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Run basic agents on scenarios and get flow results in dataframe</a></span></li><li><span><a href=\"#saving-the-data-in-a-single-dataframe\" data-toc-modified-id=\"saving-the-data-in-a-single-dataframe-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>saving the data in a single dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#check-Energy-mix\" data-toc-modified-id=\"check-Energy-mix-1.3.2.1\"><span class=\"toc-item-num\">1.3.2.1&nbsp;&nbsp;</span>check Energy mix</a></span></li></ul></li><li><span><a href=\"#Bonus:-build-specific-dataframe-on-overloads-based-on-flow-results-&amp;-a-lighter-df-analysis\" data-toc-modified-id=\"Bonus:-build-specific-dataframe-on-overloads-based-on-flow-results-&amp;-a-lighter-df-analysis-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Bonus: build specific dataframe on overloads based on flow results &amp; a lighter df analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#summary-overload-dataframe\" data-toc-modified-id=\"summary-overload-dataframe-1.3.3.1\"><span class=\"toc-item-num\">1.3.3.1&nbsp;&nbsp;</span>summary overload dataframe</a></span></li><li><span><a href=\"#ligther-df_analysis\" data-toc-modified-id=\"ligther-df_analysis-1.3.3.2\"><span class=\"toc-item-num\">1.3.3.2&nbsp;&nbsp;</span>ligther df_analysis</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import grid2op\n",
    "import cufflinks as cf\n",
    "from grid2op.PlotGrid import PlotMatplot\n",
    "import pyarrow #necessary for saving dataframe in feather format\n",
    "from grid2op.Parameters import Parameters\n",
    "from grid2op.Chronics import Multifolder, GridStateFromFileWithForecasts\n",
    "from lightsim2grid import LightSimBackend\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "#from grid2op.PlotGrid import PlotMatplot\n",
    "from grid2op.PlotGrid import NUKE_COLOR, THERMAL_COLOR, HYDRO_COLOR, SOLAR_COLOR, WIND_COLOR\n",
    "import matplotlib.pyplot as matplt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config parameters for data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged as `parameters`\n",
    "# can be used with papermill for parametrized notebook execution in cli\n",
    "env_mix_name=\"l2rpn_neurips_2020_track2_x3\"\n",
    "\n",
    "input_folder=\"Inputs\" #where to load some data\n",
    "output_folder=\"Outputs\" #where to save generated dataframes\n",
    "\n",
    "n_scenarios_to_look_at=120#2#120 #number of scenarios to run\n",
    "random_seed=0\n",
    "nb_process=10 #nb of cores to use when running scenarios\n",
    "\n",
    "action_file_name=\"l2rpn_wcci_actions.json\" #path of file with unitary action agents to run\n",
    "only_do_nothing_agent=False #if you only want to run do nothing agent, and not other unitary action agents\n",
    "\n",
    "env_name = \"l2rpn_neurips_2020_track2_small\"#'l2rpn_idf_2023_v0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_chronix = f\"/home/marotant/data_grid2op/{env_name}/{env_mix_name}/chronics/\"#f\"../generation_donnees/{env_name}/chronics/\"\n",
    "path_env = f\"/home/marotant/data_grid2op/{env_name}\"#f\"../generation_donnees/{env_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Parameters()\n",
    "params.NO_OVERFLOW_DISCONNECTION = True\n",
    "env_ref = grid2op.make(path_env,\n",
    "                   chronics_path=path_chronix,\n",
    "                   param=params,\n",
    "                   backend=LightSimBackend(),\n",
    "                   chronics_class=Multifolder,\n",
    "                   data_feeding_kwargs={\"gridvalueClass\": GridStateFromFileWithForecasts}\n",
    "                   )\n",
    "env_ref_ = env_ref[env_mix_name]\n",
    "#obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load actions\n",
    "from utils.OneChangeAgent import load_actions\n",
    "if only_do_nothing_agent:\n",
    "    actions=[{}]#only do nothing action\n",
    "else:\n",
    "    actions=load_actions(os.path.join(input_folder,action_file_name))\n",
    "    actions.insert(0,{})#add do nothing\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run basic agents on scenarios and get flow results in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from grid2op.Agent import DoNothingAgent\n",
    "from grid2op.Runner import Runner\n",
    "from utils.Runner import Runner_Calibration\n",
    "from utils.OneChangeAgent import OneChangeThenOnlyReconnect, load_actions, set_agent_name\n",
    "from utils.Create_Run_Dataframe import create_run_df,get_size\n",
    "import time\n",
    "\n",
    "obs_var_to_keep=[\"a_or\",\"load_p\",\"gen_p\",\"day_of_week\",\"hour_of_day\",\"month\"]\n",
    "#list_df=[]\n",
    "\n",
    "env_output_folder=os.path.join(output_folder,env_mix_name)\n",
    "if not os.path.exists(env_output_folder):\n",
    "    os.makedirs(env_output_folder)\n",
    "\n",
    "agent_names=[]\n",
    "\n",
    "counter_folders=0#check that a folder is not larger than a certain size or create a new one\n",
    "max_folder_size_gb=7#0.05#7\n",
    "\n",
    "current_env_output_folder=os.path.join(env_output_folder,'FlowDataset_'+str(counter_folders))\n",
    "os.makedirs(current_env_output_folder,exist_ok=True)\n",
    "\n",
    "df_analysis=pd.DataFrame()\n",
    "\n",
    "for i,act in enumerate(actions):#one OneChangeThenOnlyReconnect agent running per action to test\n",
    "    \n",
    "    action=env_ref_.action_space(act)\n",
    "    agent_name=set_agent_name(action,agent_names)\n",
    "    agent_names.append(agent_name)\n",
    "    print(\"agent number: \"+str(i))    \n",
    "    print(agent_name)\n",
    "    print(action)\n",
    "    \n",
    "\n",
    "    df_file_path=os.path.join(current_env_output_folder,'FlowDataset_'+agent_name+'.parket')\n",
    "    if (os.path.exists(df_file_path)):#already exist in the current folder, so load it\n",
    "        print(\"file exist: \"+df_file_path)\n",
    "        #agent_df=pd.read_parquet(df_file_path)\n",
    "        continue\n",
    "    else:\n",
    "        if(get_size(current_env_output_folder)>max_folder_size_gb*10**9):#switch folder\n",
    "            counter_folders+=1\n",
    "            current_env_output_folder=os.path.join(env_output_folder,'FlowDataset_'+str(counter_folders))\n",
    "            os.makedirs(current_env_output_folder,exist_ok=True)\n",
    "            df_file_path=os.path.join(current_env_output_folder,'FlowDataset_'+agent_name+'.parket')\n",
    "            \n",
    "        if (os.path.exists(df_file_path)):#exist in switched folder, so load it\n",
    "            print(\"file exist: \"+df_file_path)\n",
    "            #agent_df=pd.read_parquet(df_file_path)\n",
    "            continue\n",
    "        else:#else compute the run for this agent\n",
    "            oneAction_agent_class = OneChangeThenOnlyReconnect.gen_next(action)\n",
    "            runner_ref = Runner_Calibration(obs_var_to_keep,**env_ref_.get_params_for_runner(), agentClass=oneAction_agent_class)#Runner_Calibration\n",
    "            res_ref = runner_ref.run(nb_episode=n_scenarios_to_look_at,nb_process=nb_process,add_detailed_output=True)\n",
    "\n",
    "            agent_df=create_run_df(env_ref,env_ref_.env_name,res_ref,agent_name,available_obs=obs_var_to_keep)\n",
    "            print(\"saving agent results\")\n",
    "            agent_df.to_parquet(df_file_path,compression='brotli')\n",
    "\n",
    "    #print(\"appending agent results to df\")\n",
    "    #start_time=time.time()\n",
    "    #df_analysis=df_analysis.append(agent_df)#use append because faster that concat in our case\n",
    "    #end_time=time.time()\n",
    "    #print(\"appending dataframe took: \"+str(end_time-start_time))\n",
    "    #list_df.append(create_run_df(env_ref,env_ref_.env_name,res_ref,agent_name,available_obs=obs_var_to_keep))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved dataframes per chunk each time one was computed not to lose this computation if the kernel dies\n",
    "\n",
    "We have used the .parket file format to allow for easy files merging at reloading\n",
    "\n",
    "We are then creating one overall parket file per chunk folder\n",
    "\n",
    "And finally one full dataframe file for all results \n",
    "\n",
    "This goes step by step to run this with limited RAM memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here are the folders and files created\")\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "          \n",
    "list_files(env_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the data in a single dataframe\n",
    "creating one parquet dataframe file per chunk folders FlowDataset_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating one parquet dataframe file per chunk folders FlowDataset_i\n",
    "from utils.Create_Run_Dataframe import save_light_df_file\n",
    "for directory in os.listdir(env_output_folder):\n",
    "    #\n",
    "    if os.path.isdir(os.path.join(env_output_folder,directory)):\n",
    "        print(directory)\n",
    "        dir_output_folder=os.path.join(env_output_folder,directory)\n",
    "        save_light_df_file(dir_output_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a fully merged dataframe from each FlowDataset_i.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_analysis=pd.DataFrame()\n",
    "df_analysis_file_path=os.path.join(env_output_folder,'FlowDataset.file')\n",
    "\n",
    "if(os.path.isfile(df_analysis_file_path)):\n",
    "    print(\"loading the file from: \"+df_analysis_file_path)\n",
    "    df_analysis=pd.read_feather(df_analysis_file_path)\n",
    "    if only_do_nothing_agent:\n",
    "        df_analysis=df_analysis[df_analysis.agent==\"do_nothing\"]\n",
    "        df_analysis.reset_index(drop=True,inplace=True)\n",
    "\n",
    "else:\n",
    "    for path in os.listdir(env_output_folder):\n",
    "        if os.path.isfile(os.path.join(env_output_folder,path)) and (\".file\" in path):\n",
    "            print(path)\n",
    "            df_analysis=df_analysis.append(pd.read_feather(os.path.join(env_output_folder,path)))#.reset_index(drop=True)\n",
    "            #df_analysis.reset_index(drop=True)\n",
    "    #df_analysis.to_feather(os.path.join(env_output_folder,'FlowDataset.file'))\n",
    "\n",
    "    df_analysis.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_analysis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if(os.path.isfile(df_analysis_file_path)):\n",
    "    print(\"this file already exist: \"+df_analysis_file_path)\n",
    "else:\n",
    "    print(\"saving file in: \"+df_analysis_file_path)\n",
    "    df_analysis.to_feather(df_analysis_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check Energy mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_df=df_analysis[[\"hydro\",\"wind\",\"solar\",\"nuclear\",\"thermal\"]].astype(\"float32\").sum()\n",
    "mix_df.plot.pie(autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: build specific dataframe on overloads based on flow results & a lighter df analysis\n",
    "in case base thermal limits already exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summary overload dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Create_Run_Dataframe import fast_pd_concat,get_overload_info_df\n",
    "\n",
    "#df_analysis=fast_pd_concat(list_df)\n",
    "lines_name=env_ref_.name_line\n",
    "thermal_limits=1.0 * env_ref._thermal_limit_a\n",
    "indicesLineOverloaded=[i  for i,l in enumerate(env_ref_.name_line) if (df_analysis[l]>=env_ref._thermal_limit_a[i]).sum()>=1]\n",
    "#np.where((df_analysis[env_ref_.name_line]>=env_ref._thermal_limit_a).sum(axis=0)>=1)[0] #too memory costly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overload_file_path=os.path.join(env_output_folder,'Overloads_info_Agents.file')\n",
    "\n",
    "if(os.path.isfile(overload_file_path)):\n",
    "    print(\"loading the file from: \"+overload_file_path)\n",
    "    overloads_info=pd.read_feather(overload_file_path)\n",
    "    if only_do_nothing_agent:\n",
    "        overloads_info=overloads_info[overloads_info.agent==\"do_nothing\"].reset_index(drop=True)\n",
    "\n",
    "else:\n",
    "    overloads_info=get_overload_info_df(df_analysis,lines_name,thermal_limits,indicesLineOverloaded, verbose=True)\n",
    "    print(\"saving file in: \"+overload_file_path)\n",
    "    overloads_info.to_feather(overload_file_path)\n",
    "\n",
    "overloads_info.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ligther df_analysis\n",
    "Given that we know the number overload at each timesteps, for each agent, we will take the best of all agents at each time step to identify what could have been the best configuration with least overloads. The result will be a \"best_agent\". We can then only keep the \"do_nothing\" agent and \"best_agent\" for a more straightforward analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overload_df(df_analysis,line_names,thermal_limits):\n",
    "    nb_overloads_df=pd.DataFrame({'nb_total':np.zeros(df_analysis.shape[0])})\n",
    "\n",
    "    for i,l in enumerate(line_names):\n",
    "        print(\"computing for line \" + l)\n",
    "        nb_overloads_df[\"nb_total\"]=nb_overloads_df[\"nb_total\"].add((df_analysis[l]>= thermal_limits[i]).astype('bool'))\n",
    "    \n",
    "    nb_overloads_df[\"scenario\"]=df_analysis[\"scenario\"]\n",
    "    nb_overloads_df[\"agent\"]=df_analysis[\"agent\"]\n",
    "    nb_overloads_df[\"datetimes\"]=df_analysis[\"datetimes\"]\n",
    "    \n",
    "    #plot per agent\n",
    "    matplt.xticks(rotation=90)\n",
    "    nb_overloads_df.groupby([\"agent\"]).sum().plot(kind=\"bar\",title=\"number of overloads per agent over all scenarios\")\n",
    "    \n",
    "    \n",
    "    return nb_overloads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thermal_limits=env_ref_._thermal_limit_a.astype('float16')\n",
    "line_names=env_ref.name_line\n",
    "\n",
    "Overloads_df=get_overload_df(df_analysis,line_names,thermal_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df=Overloads_df.sort_values('nb_total', ascending=True).drop_duplicates(['scenario','datetimes'])\n",
    "df_analysis_light=df_analysis.loc[best_df.index,:]\n",
    "df_analysis_light[\"agent\"]=\"best_agent\"\n",
    "\n",
    "df_analysis_light=df_analysis_light.append(df_analysis[df_analysis.agent==\"do_nothing\"]).reset_index(drop=True)\n",
    "\n",
    "df_analysis_light_file_path=os.path.join(env_output_folder,'FlowDataset_light.file')\n",
    "df_analysis_light.to_feather(df_analysis_light_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_reduced=np.round(1-(best_df[\"nb_total\"].sum()/Overloads_df.loc[Overloads_df.agent==\"do_nothing\",\"nb_total\"].sum()),2)*100\n",
    "print(\"the percentage of overload decrease with considered actions is: \"+str(percentage_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overload_best_agent_file_path=os.path.join(env_output_folder,'Overloads_info_Best_Agent.file')\n",
    "\n",
    "if(os.path.isfile(overload_best_agent_file_path)):\n",
    "    print(\"loading the file from: \"+overload_best_agent_file_path)\n",
    "    overloads_info_best_agent=pd.read_feather(overload_file_path)\n",
    "    if only_do_nothing_agent:\n",
    "        overloads_info_best_agent=overloads_info_best_agent[overloads_info_best_agent.agent==\"do_nothing\"].reset_index(drop=True)\n",
    "\n",
    "else:\n",
    "    overloads_info_best_agent=get_overload_info_df(df_analysis_light,lines_name,thermal_limits,indicesLineOverloaded, verbose=True)\n",
    "    print(\"saving file in: \"+overload_best_agent_file_path)\n",
    "    overloads_info_best_agent.to_feather(overload_best_agent_file_path)\n",
    "\n",
    "overloads_info_best_agent.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "216px",
    "width": "438px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "34716039149a12aac9be247e523169e2e51f477b8e19a8020c663512dece4b8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
